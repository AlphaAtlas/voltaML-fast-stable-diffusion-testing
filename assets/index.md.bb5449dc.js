import{_ as t,c as e,o as a,d as o}from"./app.7c78610c.js";const r="/voltaML-fast-stable-diffusion/assets/frontend-txt2img.29b0b546.webp",s="/voltaML-fast-stable-diffusion/assets/frontend-img2img.6fbc5e01.webp",i="/voltaML-fast-stable-diffusion/assets/frontend-browser.98c76dd0.webp",_=JSON.parse('{"title":"Welcome to VoltaML","description":"","frontmatter":{},"headers":[{"level":2,"title":"Main features","slug":"main-features","link":"#main-features","children":[]},{"level":2,"title":"Speed comparison","slug":"speed-comparison","link":"#speed-comparison","children":[]},{"level":2,"title":"UI Preview","slug":"ui-preview","link":"#ui-preview","children":[]}],"relativePath":"index.md","lastUpdated":1676817081000}'),d={name:"index.md"},n=o('<h1 id="welcome-to-voltaml" tabindex="-1">Welcome to VoltaML <a class="header-anchor" href="#welcome-to-voltaml" aria-hidden="true">#</a></h1><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Documentation is still a work in progress, if you have any questions, feel free to join our <a href="https://discord.gg/pY5SVyHmWm" target="_blank" rel="noreferrer">Discord server</a> or open an issue on GitHub.</p></div><p>Stable Diffusion WebUI and API accelerated by <a href="https://developer.nvidia.com/tensorrt">TensorRT</a></p><p><strong>This documentation should walk you through the installation process, your first generated image, setting up the project to your liking and accelerating models with TensorRT.</strong></p><p>**There is also a dedicated section to the Discord bot, API and a section for **developers and collaborators.****</p><h2 id="main-features" tabindex="-1">Main features <a class="header-anchor" href="#main-features" aria-hidden="true">#</a></h2><ul><li>Easy install with Docker</li><li>Clean and simple Web UI</li><li>Supports PyTorch as well as TensorRT for the fastest inference</li><li>Support for Windows and Linux (TRT is not officially supported on Windows if running locally)</li><li>xFormers support</li><li>GPU cluster support with load balancing</li><li>Discord bot</li></ul><h2 id="speed-comparison" tabindex="-1">Speed comparison <a class="header-anchor" href="#speed-comparison" aria-hidden="true">#</a></h2><div class="warning custom-block"><p class="custom-block-title">WARNING</p><p>Old data, in need of rerun - observed speedup should be approximately 2.5x</p></div><p>The below benchmarks have been done for generating a 512x512 image, batch size of one (measured in it/s).</p><table><thead><tr><th>GPU</th><th>PyTorch</th><th>xFormers</th><th>TensorRT</th></tr></thead><tbody><tr><td>RTX 4090</td><td>19</td><td>40</td><td>87</td></tr><tr><td>RTX 2080 Ti</td><td>8</td><td>No data</td><td>26.2</td></tr><tr><td>RTX 3050</td><td>4.6</td><td>5.7</td><td>12.5</td></tr><tr><td>A100</td><td>15.1</td><td>27.5</td><td>62.8</td></tr><tr><td>A10</td><td>8.8</td><td>15.6</td><td>29.2</td></tr><tr><td>T4</td><td>4.3</td><td>5.5</td><td>11.4</td></tr></tbody></table><h2 id="ui-preview" tabindex="-1">UI Preview <a class="header-anchor" href="#ui-preview" aria-hidden="true">#</a></h2><p><strong>Text to image</strong><img src="'+r+'" alt="Text2Image"></p><hr><p><strong>Image to image</strong><img src="'+s+'" alt="Image2Image"></p><hr><p><strong>Image Browser</strong><img src="'+i+'" alt="ImageBrowser"></p>',17),l=[n];function c(p,h,m,u,f,g){return a(),e("div",null,l)}const T=t(d,[["render",c]]);export{_ as __pageData,T as default};
