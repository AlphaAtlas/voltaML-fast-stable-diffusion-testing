import{_ as t,c as e,o as a,a as o}from"./app.0ffd4db5.js";const r="/voltaML-fast-stable-diffusion/assets/frontend-txt2img.29b0b546.webp",d="/voltaML-fast-stable-diffusion/assets/frontend-img2img.6fbc5e01.webp",i="/voltaML-fast-stable-diffusion/assets/frontend-browser.98c76dd0.webp",_=JSON.parse('{"title":"Welcome to VoltaML","description":"","frontmatter":{},"headers":[{"level":2,"title":"Main features","slug":"main-features","link":"#main-features","children":[]},{"level":2,"title":"Speed comparison","slug":"speed-comparison","link":"#speed-comparison","children":[]},{"level":2,"title":"UI Preview","slug":"ui-preview","link":"#ui-preview","children":[]}],"relativePath":"introduction.md","lastUpdated":1679138189000}'),s={name:"introduction.md"},n=o('<h1 id="welcome-to-voltaml" tabindex="-1">Welcome to VoltaML <a class="header-anchor" href="#welcome-to-voltaml" aria-hidden="true">#</a></h1><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Documentation is still a work in progress, if you have any questions, feel free to join our <a href="https://discord.gg/pY5SVyHmWm" target="_blank" rel="noreferrer">Discord server</a> or open an issue on GitHub.</p></div><p>Stable Diffusion WebUI accelerated by <a href="https://github.com/facebookincubator/AITemplate">AITemplate</a></p><p><strong>This documentation should walk you through the installation process, your first generated image, setting up the project to your liking and accelerating models with AITemplate.</strong></p><p>There is also a dedicated section to the <strong>Discord bot, API</strong> and a section for <strong>developers and collaborators.</strong></p><h2 id="main-features" tabindex="-1">Main features <a class="header-anchor" href="#main-features" aria-hidden="true">#</a></h2><ul><li>Easy install with Docker</li><li>Clean and simple Web UI</li><li>Supports PyTorch as well as AITemplat for inference</li><li>Support for Windows and Linux</li><li>xFormers supported out of the box</li><li>GPU cluster load balancing</li><li>Discord bot</li><li>Documented API</li><li>Clean source code that should be easy to understand</li></ul><h2 id="speed-comparison" tabindex="-1">Speed comparison <a class="header-anchor" href="#speed-comparison" aria-hidden="true">#</a></h2><p>The below benchmarks have been done for generating a 512x512 image, batch size of one, measured in it/s.</p><table><thead><tr><th>GPU</th><th>PyTorch</th><th>xFormers</th><th>AITemplate</th></tr></thead><tbody><tr><td>RTX 4090</td><td>19</td><td>40</td><td>No data</td></tr><tr><td>RTX 2080 Ti</td><td>8</td><td>No data</td><td>No data</td></tr><tr><td>RTX 3050</td><td>4.6</td><td>5.7</td><td>10.15</td></tr><tr><td>RTX 3060 Ti</td><td>No data</td><td>10.50</td><td>19.46</td></tr><tr><td>A100</td><td>15.1</td><td>27.5</td><td>No data</td></tr><tr><td>A10</td><td>8.8</td><td>15.6</td><td>23.5</td></tr><tr><td>T4</td><td>4.3</td><td>5.5</td><td>No data</td></tr></tbody></table><h2 id="ui-preview" tabindex="-1">UI Preview <a class="header-anchor" href="#ui-preview" aria-hidden="true">#</a></h2><p><strong>Text to image</strong><img src="'+r+'" alt="Text2Image"></p><hr><p><strong>Image to image</strong><img src="'+d+'" alt="Image2Image"></p><hr><p><strong>Image Browser</strong><img src="'+i+'" alt="ImageBrowser"></p>',16),l=[n];function c(p,h,m,u,f,g){return a(),e("div",null,l)}const T=t(s,[["render",c]]);export{_ as __pageData,T as default};
